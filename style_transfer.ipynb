{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e78b61a",
   "metadata": {},
   "source": [
    "# Style Transfer for robot movements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e671f9",
   "metadata": {},
   "source": [
    "Uses robot movements (from CSV files) and human movement videos (ballet, jazz, etc.) \n",
    "\n",
    "Learns human movement styles by analysing both raw video and pose using mediapipe\n",
    "\n",
    "Transfers the style of human movement to robot motions while keeping them similar\n",
    "\n",
    "Few-shot adaptation is used to adapt to new styles with a few example videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b1957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "video_folder = \"robot_motion_data/human_videos\" #Folder containing human motion videos\n",
    "robot_folder = \"robot_motion_data/robot_motions2/\" #Folder containing robot 7DOF joint CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c540e4d7",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537a7455",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_robot_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    joint_columns = ['Position1', 'Position2', 'Position3', 'Position4', \n",
    "                    'Position5', 'Position6', 'Position7']\n",
    "    joint_data = df[joint_columns].values.astype(np.float32)\n",
    "    joint_data = joint_data[~np.isnan(joint_data).any(axis=1)]\n",
    "    return joint_data\n",
    "\n",
    "def create_robot_sequences(joint_data, sequence_length=128):\n",
    "    sequences = []\n",
    "    for i in range(0, len(joint_data) - sequence_length + 1, sequence_length // 2):\n",
    "        sequence = joint_data[i:i + sequence_length]\n",
    "        sequences.append(sequence)\n",
    "    return np.array(sequences, dtype=np.float32)\n",
    "\n",
    "def process_robot_folder(folder_path, sequence_length=128):\n",
    "    csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "    all_sequences = []\n",
    "    all_raw_data = []\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        joint_data = load_robot_csv(csv_file)\n",
    "        all_raw_data.append(joint_data)\n",
    "        sequences = create_robot_sequences(joint_data, sequence_length)\n",
    "        all_sequences.extend(sequences)\n",
    "    \n",
    "    # Fit scaler on all data\n",
    "    combined_data = np.vstack(all_raw_data).astype(np.float32)\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(combined_data)\n",
    "    \n",
    "    # Scale sequences\n",
    "    scaled_sequences = []\n",
    "    for seq in all_sequences:\n",
    "        scaled_seq = scaler.transform(seq.astype(np.float32))\n",
    "        scaled_sequences.append(scaled_seq.astype(np.float32))\n",
    "    \n",
    "    sequences = np.array(scaled_sequences, dtype=np.float32)\n",
    "    return sequences, scaler\n",
    "\n",
    "def load_video(video_path, target_size=(64, 64), sequence_length=128):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    \n",
    "    while len(frames) < sequence_length:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, target_size)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = frame.astype(np.float32) / 255.0\n",
    "        frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Pad if necessary\n",
    "    while len(frames) < sequence_length:\n",
    "        frames.append(frames[-1])\n",
    "    \n",
    "    return np.array(frames[:sequence_length])\n",
    "\n",
    "def extract_pose_sequence(video_frames):\n",
    "    mp_pose = mp.solutions.pose.Pose(\n",
    "        model_complexity=2,\n",
    "        enable_segmentation=False,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "    \n",
    "    pose_sequences = []\n",
    "    for frame in video_frames:\n",
    "        frame_uint8 = (frame * 255).astype(np.uint8)\n",
    "        results = mp_pose.process(frame_uint8)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            keypoints = []\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                keypoints.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
    "            pose_sequences.append(keypoints)\n",
    "        else:\n",
    "            pose_sequences.append([0.0] * 132)  # 33 landmarks Ã— 4 coordinates\n",
    "    \n",
    "    mp_pose.close()\n",
    "    return np.array(pose_sequences, dtype=np.float32)\n",
    "\n",
    "def process_video_folder_with_poses(video_folder, pose_folder=None):\n",
    "    if pose_folder is None:\n",
    "        pose_folder = os.path.join(video_folder, \"..\", \"preprocessed_poses\")\n",
    "    \n",
    "    os.makedirs(pose_folder, exist_ok=True)\n",
    "    video_pose_mapping = {}\n",
    "    \n",
    "    for style_name in os.listdir(video_folder):\n",
    "        style_path = os.path.join(video_folder, style_name)\n",
    "        if not os.path.isdir(style_path):\n",
    "            continue\n",
    "        \n",
    "        style_pose_folder = os.path.join(pose_folder, style_name)\n",
    "        os.makedirs(style_pose_folder, exist_ok=True)\n",
    "        \n",
    "        video_files = []\n",
    "        for ext in ['*.mp4', '*.avi', '*.mov', '*.mkv']:\n",
    "            video_files.extend(glob.glob(os.path.join(style_path, ext)))\n",
    "        \n",
    "        if not video_files:\n",
    "            continue\n",
    "        \n",
    "        video_pose_pairs = []\n",
    "        for video_path in tqdm(video_files, desc=f\"Processing {style_name}\"):\n",
    "            try:\n",
    "                video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "                pose_file_path = os.path.join(style_pose_folder, f\"{video_name}_poses.npy\")\n",
    "                \n",
    "                if os.path.exists(pose_file_path):\n",
    "                    video_pose_pairs.append((video_path, pose_file_path))\n",
    "                    continue\n",
    "                \n",
    "                video_frames = load_video(video_path)\n",
    "                poses = extract_pose_sequence(video_frames)\n",
    "                np.save(pose_file_path, poses.astype(np.float32))\n",
    "                video_pose_pairs.append((video_path, pose_file_path))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {os.path.basename(video_path)}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if video_pose_pairs:\n",
    "            video_pose_mapping[style_name] = video_pose_pairs\n",
    "    \n",
    "    # Save mapping\n",
    "    mapping_path = os.path.join(pose_folder, \"video_pose_mapping.pkl\")\n",
    "    with open(mapping_path, 'wb') as f:\n",
    "        pickle.dump(video_pose_mapping, f)\n",
    "    \n",
    "    return video_pose_mapping\n",
    "\n",
    "def process_robot_data():\n",
    "    robot_sequences, scaler = process_robot_folder(robot_folder)\n",
    "    \n",
    "    np.save(\"processed_robot_sequences.npy\", robot_sequences)\n",
    "    with open(\"robot_scaler.pkl\", 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    print(f\"Processed {robot_sequences.shape[0]} robot sequences\")\n",
    "    return robot_sequences, scaler\n",
    "\n",
    "def preprocess_videos():\n",
    "    mapping = process_video_folder_with_poses(video_folder)\n",
    "    print(\"Video preprocessing complete!\")\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b8c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_robot_data()\n",
    "preprocess_videos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7295e608",
   "metadata": {},
   "source": [
    "## Model architecture - cross modal attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84904772",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_temporal_motion_features(video_input):\n",
    "    \n",
    "    # Calculate frame differences (motion) using Lambda layer\n",
    "    def compute_motion_sequence(video):\n",
    "        motion_frames = []\n",
    "        for t in range(1, 128):\n",
    "            prev_frame = video[:, t-1]\n",
    "            curr_frame = video[:, t]\n",
    "            motion = curr_frame - prev_frame\n",
    "            motion_frames.append(motion)\n",
    "        \n",
    "        # Pad first frame\n",
    "        motion_frames.insert(0, motion_frames[0])\n",
    "        return tf.stack(motion_frames, axis=1)\n",
    "    \n",
    "    motion_sequence = tf.keras.layers.Lambda(compute_motion_sequence)(video_input)\n",
    "    \n",
    "    # Extract motion features per timestep\n",
    "    motion_features = tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.Conv2D(32, (8, 8), strides=(8, 8), activation='relu')\n",
    "    )(motion_sequence)\n",
    "    \n",
    "    motion_features = tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.GlobalAveragePooling2D()\n",
    "    )(motion_features)\n",
    "    \n",
    "    return motion_features\n",
    "\n",
    "def extract_temporal_pose_features(pose_input):\n",
    "    \n",
    "    # Reshape poses to (batch, time, joints, coordinates)\n",
    "    pose_reshaped = tf.keras.layers.Reshape((128, 33, 4))(pose_input)\n",
    "    pose_coords = tf.keras.layers.Lambda(lambda x: x[:, :, :, :3])(pose_reshaped)  # Only x, y, z coordinates\n",
    "    \n",
    "    # Calculate pose velocity and acceleration using Lambda layers\n",
    "    def compute_pose_dynamics(pose_coords):\n",
    "        # Calculate pose velocity (rhythm)\n",
    "        pose_velocity = pose_coords[:, 1:] - pose_coords[:, :-1]\n",
    "        pose_velocity = tf.pad(pose_velocity, [[0, 0], [0, 1], [0, 0], [0, 0]], mode='CONSTANT')\n",
    "        \n",
    "        # Calculate pose acceleration (dynamics)\n",
    "        pose_acceleration = pose_velocity[:, 1:] - pose_velocity[:, :-1]\n",
    "        pose_acceleration = tf.pad(pose_acceleration, [[0, 0], [0, 1], [0, 0], [0, 0]], mode='CONSTANT')\n",
    "        \n",
    "        return pose_velocity, pose_acceleration\n",
    "    \n",
    "    pose_velocity, pose_acceleration = tf.keras.layers.Lambda(compute_pose_dynamics)(pose_coords)\n",
    "    \n",
    "    # Extract movement magnitude and direction per timestep\n",
    "    velocity_magnitude = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(tf.abs(x), axis=[2, 3]))(pose_velocity)\n",
    "    acceleration_magnitude = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(tf.abs(x), axis=[2, 3]))(pose_acceleration)\n",
    "    \n",
    "    # Extract joint coordination patterns\n",
    "    def compute_coordination_features(pose_velocity):\n",
    "        coordination_features = []\n",
    "        for joint_idx in range(0, 33, 4):  # Sample every 4th joint\n",
    "            joint_vel = pose_velocity[:, :, joint_idx, :]\n",
    "            joint_coordination = tf.reduce_mean(joint_vel, axis=-1)  # [batch, time]\n",
    "            coordination_features.append(joint_coordination)\n",
    "        return tf.stack(coordination_features, axis=-1)  # [batch, time, 8]\n",
    "    \n",
    "    coordination_tensor = tf.keras.layers.Lambda(compute_coordination_features)(pose_velocity)\n",
    "    \n",
    "    # Combine rhythm features\n",
    "    def combine_rhythm_features(inputs):\n",
    "        velocity_mag, acceleration_mag = inputs\n",
    "        return tf.stack([velocity_mag, acceleration_mag], axis=-1)\n",
    "    \n",
    "    rhythm_features = tf.keras.layers.Lambda(combine_rhythm_features)([velocity_magnitude, acceleration_magnitude])\n",
    "    \n",
    "    # Combine all temporal features\n",
    "    temporal_features = tf.keras.layers.Concatenate(axis=-1)([rhythm_features, coordination_tensor])\n",
    "    \n",
    "    return temporal_features\n",
    "\n",
    "class CrossModalAttentionLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, d_model=128, num_heads=4, rate=0.1, **kwargs):\n",
    "        super(CrossModalAttentionLayer, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.rate = rate\n",
    "        \n",
    "        # Cross-attention layers\n",
    "        self.video_to_pose_attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=d_model//num_heads, dropout=rate\n",
    "        )\n",
    "        self.pose_to_video_attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=d_model//num_heads, dropout=rate\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm4 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Projection layers to ensure consistent dimensions\n",
    "        self.video_projection = tf.keras.layers.Dense(d_model)\n",
    "        self.pose_projection = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        video_features, pose_features = inputs\n",
    "        \n",
    "        # Project to consistent dimensions\n",
    "        video_proj = self.video_projection(video_features)\n",
    "        pose_proj = self.pose_projection(pose_features)\n",
    "        \n",
    "        # Video attending to pose (video queries, pose keys/values)\n",
    "        video_attended = self.video_to_pose_attention(\n",
    "            query=video_proj, key=pose_proj, value=pose_proj, training=training\n",
    "        )\n",
    "        video_attended = self.dropout1(video_attended, training=training)\n",
    "        video_out = self.layernorm1(video_proj + video_attended)  # Residual connection\n",
    "        \n",
    "        # Pose attending to video (pose queries, video keys/values)\n",
    "        pose_attended = self.pose_to_video_attention(\n",
    "            query=pose_proj, key=video_proj, value=video_proj, training=training\n",
    "        )\n",
    "        pose_attended = self.dropout2(pose_attended, training=training)\n",
    "        pose_out = self.layernorm2(pose_proj + pose_attended)  # Residual connection\n",
    "        \n",
    "        return video_out, pose_out\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'd_model': self.d_model,\n",
    "            'num_heads': self.num_heads,\n",
    "            'rate': self.rate\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "def build_temporal_style_encoder(embed_dim=256, use_cross_attention=True):\n",
    "    \n",
    "    video_input = tf.keras.Input(shape=(128, 64, 64, 3), name='video_frames')\n",
    "    pose_input = tf.keras.Input(shape=(128, 132), name='pose_keypoints')\n",
    "    \n",
    "    # Extract temporal features\n",
    "    video_temporal = extract_temporal_motion_features(video_input)\n",
    "    pose_temporal = extract_temporal_pose_features(pose_input)\n",
    "    \n",
    "    print(f\"Video temporal shape expected: [batch, 128, 32]\")\n",
    "    print(f\"Pose temporal shape expected: [batch, 128, 10]\")\n",
    "    \n",
    "    if use_cross_attention:\n",
    "        # Apply cross-modal attention\n",
    "        cross_attention = CrossModalAttentionLayer(d_model=128, num_heads=4, rate=0.1)\n",
    "        video_attended, pose_attended = cross_attention([video_temporal, pose_temporal])\n",
    "        \n",
    "        # Additional processing after attention\n",
    "        video_processed = tf.keras.layers.Conv1D(64, 3, padding='same', activation='relu')(video_attended)\n",
    "        pose_processed = tf.keras.layers.Conv1D(64, 3, padding='same', activation='relu')(pose_attended)\n",
    "        \n",
    "        # Combine attended features\n",
    "        combined_features = tf.keras.layers.Concatenate(axis=-1)([video_processed, pose_processed])\n",
    "        \n",
    "        # Apply attention to the combined features to focus on important temporal moments\n",
    "        temporal_attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=8, key_dim=16, dropout=0.1\n",
    "        )(combined_features, combined_features)\n",
    "        \n",
    "        # Residual connection and normalization\n",
    "        attended_combined = tf.keras.layers.LayerNormalization()(\n",
    "            combined_features + temporal_attention\n",
    "        )\n",
    "        \n",
    "        # Final processing\n",
    "        style_conv1 = tf.keras.layers.Conv1D(128, 5, padding='same', activation='relu')(attended_combined)\n",
    "        style_conv2 = tf.keras.layers.Conv1D(128, 3, padding='same', activation='relu')(style_conv1)\n",
    "        \n",
    "    else:\n",
    "        # Fallback to simple concatenation\n",
    "        combined_temporal = tf.keras.layers.Concatenate(axis=-1)([video_temporal, pose_temporal])\n",
    "        \n",
    "        # Process with temporal convolutions\n",
    "        style_conv1 = tf.keras.layers.Conv1D(128, 5, padding='same', activation='relu')(combined_temporal)\n",
    "        style_conv2 = tf.keras.layers.Conv1D(128, 3, padding='same', activation='relu')(style_conv1)\n",
    "    \n",
    "    # Apply attention to focus on important temporal moments\n",
    "    attention_weights = tf.keras.layers.Dense(1, activation='sigmoid')(style_conv2)\n",
    "    style_attended = tf.keras.layers.Multiply()([style_conv2, attention_weights])\n",
    "    \n",
    "    # Final temporal style features\n",
    "    style_features = tf.keras.layers.Dense(embed_dim, activation='relu')(style_attended)\n",
    "    \n",
    "    model_name = 'temporal_style_encoder_with_attention' if use_cross_attention else 'temporal_style_encoder'\n",
    "    return tf.keras.Model([video_input, pose_input], style_features, name=model_name)\n",
    "\n",
    "class TemporalTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, feed_forward_dim, rate=0.1):\n",
    "        super(TemporalTransformerBlock, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.feed_forward_dim = feed_forward_dim\n",
    "        self.rate = rate\n",
    "        \n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(feed_forward_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        attn_output = self.mha(x, x, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2\n",
    "\n",
    "def build_temporal_style_transfer_transformer(sequence_length=128, num_joints=7, style_dim=256):\n",
    "    \n",
    "    robot_sequence = tf.keras.Input(shape=(sequence_length, num_joints), name='robot_sequence')\n",
    "    style_sequence = tf.keras.Input(shape=(sequence_length, style_dim), name='style_sequence')\n",
    "    \n",
    "    # Combine robot motion with temporal style\n",
    "    combined_input = tf.keras.layers.Concatenate(axis=-1)([robot_sequence, style_sequence])\n",
    "    \n",
    "    # Project to model dimension\n",
    "    d_model = 256\n",
    "    x = tf.keras.layers.Dense(d_model)(combined_input)\n",
    "    \n",
    "    # Add positional encoding\n",
    "    def add_positional_encoding(inputs):\n",
    "        x_input = inputs\n",
    "        positions = tf.range(sequence_length, dtype=tf.float32)\n",
    "        pos_encoding = tf.keras.layers.Embedding(sequence_length, d_model)(positions)\n",
    "        return x_input + pos_encoding\n",
    "    \n",
    "    x = tf.keras.layers.Lambda(add_positional_encoding)(x)\n",
    "    \n",
    "    # Multi-layer transformer with moderate dropout\n",
    "    x = TemporalTransformerBlock(d_model=d_model, num_heads=8, feed_forward_dim=512, rate=0.25)(x)\n",
    "    x = TemporalTransformerBlock(d_model=d_model, num_heads=8, feed_forward_dim=512, rate=0.25)(x)\n",
    "    x = TemporalTransformerBlock(d_model=d_model, num_heads=8, feed_forward_dim=512, rate=0.25)(x)\n",
    "    \n",
    "    # Generate raw residuals\n",
    "    residual_raw = tf.keras.layers.Dense(num_joints, activation='tanh')(x)\n",
    "    \n",
    "    # Smoothing\n",
    "    # Stage 1: Light temporal smoothing (preserve detail)\n",
    "    residual_smooth1 = tf.keras.layers.Conv1D(num_joints, 5, padding='same', activation='linear', \n",
    "                                             kernel_initializer='ones', use_bias=False)(residual_raw)\n",
    "    residual_smooth1 = tf.keras.layers.Lambda(lambda x: x / 5.0)(residual_smooth1)  # Normalize\n",
    "    \n",
    "    # Stage 2: Medium temporal smoothing (reduce jitter)\n",
    "    residual_smooth2 = tf.keras.layers.Conv1D(num_joints, 9, padding='same', activation='linear',\n",
    "                                             kernel_initializer='ones', use_bias=False)(residual_raw)\n",
    "    residual_smooth2 = tf.keras.layers.Lambda(lambda x: x / 9.0)(residual_smooth2)  # Normalize\n",
    "    \n",
    "    # Stage 3: Heavy temporal smoothing (eliminate rapid changes)\n",
    "    residual_smooth3 = tf.keras.layers.Conv1D(num_joints, 15, padding='same', activation='linear',\n",
    "                                             kernel_initializer='ones', use_bias=False)(residual_raw)\n",
    "    residual_smooth3 = tf.keras.layers.Lambda(lambda x: x / 15.0)(residual_smooth3)  # Normalize\n",
    "    \n",
    "    # Adaptive blending: 50% raw (for creativity), 30% light smooth, 15% medium, 5% heavy\n",
    "    residuals = tf.keras.layers.Lambda(lambda inputs: \n",
    "        0.5 * inputs[0] + 0.3 * inputs[1] + 0.15 * inputs[2] + 0.05 * inputs[3]\n",
    "    )([residual_raw, residual_smooth1, residual_smooth2, residual_smooth3])\n",
    "    \n",
    "    # Additional velocity-based smoothing to prevent rapid direction changes\n",
    "    def velocity_smoothing(residuals_input):\n",
    "        # Calculate velocities\n",
    "        velocities = residuals_input[:, 1:] - residuals_input[:, :-1]\n",
    "        \n",
    "        # Smooth velocities with smaller kernel\n",
    "        velocities_smooth = tf.keras.layers.Conv1D(num_joints, 3, padding='same', \n",
    "                                                  activation='linear', use_bias=False)(\n",
    "            tf.pad(velocities, [[0, 0], [0, 1], [0, 0]], mode='CONSTANT')\n",
    "        )\n",
    "        \n",
    "        # Reconstruct positions from smoothed velocities\n",
    "        velocities_smooth = velocities_smooth[:, :-1]  \n",
    "        \n",
    "        # Integrate back to positions\n",
    "        first_position = residuals_input[:, 0:1]  # Keep first position\n",
    "        positions = tf.concat([first_position, \n",
    "                              first_position + tf.cumsum(velocities_smooth, axis=1)], axis=1)\n",
    "        return positions\n",
    "    \n",
    "    residuals = tf.keras.layers.Lambda(velocity_smoothing)(residuals)\n",
    "    \n",
    "    # Scale residuals \n",
    "    scaled_residuals = tf.keras.layers.Lambda(lambda x: x * 1.0)(residuals)\n",
    "    \n",
    "    # Bound residuals but with smoother clipping to avoid discontinuities\n",
    "    def smooth_clipping(x, min_val=-1.0, max_val=1.0):\n",
    "        return min_val + (max_val - min_val) * tf.sigmoid(x / 0.5)\n",
    "    \n",
    "    bounded_residuals = tf.keras.layers.Lambda(lambda x: smooth_clipping(x))(scaled_residuals)\n",
    "    \n",
    "    return tf.keras.Model([robot_sequence, style_sequence], bounded_residuals, name='temporal_transformer')\n",
    "\n",
    "def build_robot_encoder(embed_dim=512):\n",
    "    robot_input = tf.keras.Input(shape=(128, 7))\n",
    "    x = tf.keras.layers.LSTM(128, return_sequences=True)(robot_input)\n",
    "    x = tf.keras.layers.LSTM(embed_dim)(x)\n",
    "    return tf.keras.Model(robot_input, x, name='robot_encoder')\n",
    "\n",
    "def build_discriminator():\n",
    "    motion_input = tf.keras.Input(shape=(128, 7))\n",
    "    x = tf.keras.layers.LSTM(128, return_sequences=True)(motion_input)\n",
    "    x = tf.keras.layers.LSTM(64)(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    return tf.keras.Model(motion_input, output, name='discriminator')\n",
    "\n",
    "def build_complete_temporal_model(use_cross_attention=True):\n",
    "    \n",
    "    # Components with configurable attention\n",
    "    temporal_style_encoder = build_temporal_style_encoder(use_cross_attention=use_cross_attention)\n",
    "    robot_encoder = build_robot_encoder()\n",
    "    transformer = build_temporal_style_transfer_transformer()\n",
    "    discriminator = build_discriminator() \n",
    "    \n",
    "    # Training model inputs (fixed 128 length for training)\n",
    "    video_input = tf.keras.Input(shape=(128, 64, 64, 3), name='video')\n",
    "    pose_input = tf.keras.Input(shape=(128, 132), name='poses')\n",
    "    robot_input = tf.keras.Input(shape=(128, 7), name='robot_motion')\n",
    "    \n",
    "    # Encode temporal style features \n",
    "    style_sequence = temporal_style_encoder([video_input, pose_input])\n",
    "    \n",
    "    # Encode robot motion\n",
    "    robot_encoding = robot_encoder(robot_input)\n",
    "    \n",
    "    # Generate temporal residuals directly\n",
    "    style_residuals = transformer([robot_input, style_sequence])\n",
    "    \n",
    "    # Apply residuals to create styled motion\n",
    "    styled_motion = tf.keras.layers.Add()([robot_input, style_residuals])\n",
    "    \n",
    "    # Discriminator evaluation\n",
    "    disc_score = discriminator(styled_motion)\n",
    "    \n",
    "    # Training model\n",
    "    training_model = tf.keras.Model(\n",
    "        inputs=[video_input, pose_input, robot_input],\n",
    "        outputs={\n",
    "            'styled_motion': styled_motion,\n",
    "            'style_residual': style_residuals,\n",
    "            'style_sequence': style_sequence,\n",
    "            'robot_encoding': robot_encoding,\n",
    "            'discriminator_score': disc_score\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    components = {\n",
    "        'temporal_style_encoder': temporal_style_encoder,\n",
    "        'robot_encoder': robot_encoder,\n",
    "        'transformer': transformer,\n",
    "        'discriminator': discriminator\n",
    "    }\n",
    "    \n",
    "    return training_model, discriminator, components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53f95d5",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6e12aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_temporal_variation_loss(residuals):\n",
    "    \n",
    "    # Calculate temporal variance of residuals\n",
    "    def calculate_variance(x):\n",
    "        return tf.reduce_mean(tf.math.reduce_variance(x, axis=1))\n",
    "    \n",
    "    residual_variance = calculate_variance(residuals)\n",
    "    \n",
    "    # Encourage minimum variance (negative loss to maximize)\n",
    "    min_variance_loss = tf.maximum(0.0, 0.05 - residual_variance)\n",
    "    \n",
    "    # Calculate smoothness of residual changes\n",
    "    def calculate_jerk(x):\n",
    "        residual_velocity = x[:, 1:] - x[:, :-1]\n",
    "        residual_acceleration = residual_velocity[:, 1:] - residual_velocity[:, :-1]\n",
    "        return tf.reduce_mean(tf.square(residual_acceleration))\n",
    "    \n",
    "    jerk_penalty = calculate_jerk(residuals)\n",
    "    \n",
    "    return {\n",
    "        'temporal_variance': residual_variance,\n",
    "        'min_variance_loss': min_variance_loss,\n",
    "        'jerk_penalty': jerk_penalty\n",
    "    }\n",
    "\n",
    "def compute_style_rhythm_loss(styled_motion, original_motion, style_sequence):\n",
    "    \n",
    "    # Extract velocity from both motions\n",
    "    def extract_velocity(motion):\n",
    "        return motion[:, 1:] - motion[:, :-1]\n",
    "    \n",
    "    styled_velocity = extract_velocity(styled_motion)\n",
    "    original_velocity = extract_velocity(original_motion)\n",
    "    \n",
    "    # Calculate rhythm difference\n",
    "    def calculate_rhythm(velocity):\n",
    "        return tf.reduce_mean(tf.abs(velocity), axis=-1)\n",
    "    \n",
    "    styled_rhythm = calculate_rhythm(styled_velocity)\n",
    "    original_rhythm = calculate_rhythm(original_velocity)\n",
    "    \n",
    "    # Encourage rhythm to deviate from original\n",
    "    rhythm_deviation = tf.reduce_mean(tf.square(styled_rhythm - original_rhythm))\n",
    "    \n",
    "    # Encourage rhythm to correlate with style strength\n",
    "    style_strength = tf.reduce_mean(tf.abs(style_sequence), axis=-1)\n",
    "    style_strength_padded = style_strength[:, :-1]  # Match velocity length\n",
    "    \n",
    "    # Positive correlation between style strength and rhythm changes\n",
    "    rhythm_style_correlation = tf.reduce_mean(styled_rhythm * style_strength_padded)\n",
    "    \n",
    "    return {\n",
    "        'rhythm_deviation': rhythm_deviation,\n",
    "        'rhythm_style_correlation': rhythm_style_correlation\n",
    "    }\n",
    "\n",
    "def compute_enhanced_losses(outputs, original_robot_motion, style_sequence, style_labels=None):\n",
    "    \n",
    "    styled_motion = outputs['styled_motion']\n",
    "    style_residual = outputs['style_residual']\n",
    "    \n",
    "    losses = {}\n",
    "    \n",
    "    # Basic losses\n",
    "    losses['adversarial'] = tf.reduce_mean(tf.square(outputs['discriminator_score'] - 1.0))\n",
    "    \n",
    "    # Temporal variation losses\n",
    "    temporal_losses = compute_temporal_variation_loss(style_residual)\n",
    "    losses.update(temporal_losses)\n",
    "    \n",
    "    # Style rhythm losses\n",
    "    rhythm_losses = compute_style_rhythm_loss(styled_motion, original_robot_motion, style_sequence)\n",
    "    losses.update(rhythm_losses)\n",
    "    \n",
    "    \n",
    "    # 1. Velocity smoothness (first derivative)\n",
    "    styled_velocity = styled_motion[:, 1:] - styled_motion[:, :-1]\n",
    "    original_velocity = original_robot_motion[:, 1:] - original_robot_motion[:, :-1]\n",
    "    \n",
    "    velocity_smoothness = tf.reduce_mean(tf.square(styled_velocity))\n",
    "    original_velocity_smoothness = tf.reduce_mean(tf.square(original_velocity))\n",
    "    losses['velocity_smoothness'] = tf.maximum(0.0, velocity_smoothness - original_velocity_smoothness * 1.2)\n",
    "    \n",
    "    # 2. Acceleration smoothness (second derivative) - Key for reducing jitter\n",
    "    styled_acceleration = styled_velocity[:, 1:] - styled_velocity[:, :-1]\n",
    "    original_acceleration = original_velocity[:, 1:] - original_velocity[:, :-1]\n",
    "    \n",
    "    acceleration_smoothness = tf.reduce_mean(tf.square(styled_acceleration))\n",
    "    original_acceleration_smoothness = tf.reduce_mean(tf.square(original_acceleration))\n",
    "    losses['acceleration_smoothness'] = tf.maximum(0.0, acceleration_smoothness - original_acceleration_smoothness * 1.2)\n",
    "    \n",
    "    # 3. Jerk penalty (third derivative) - Critical for eliminating jittery motion\n",
    "    styled_jerk = styled_acceleration[:, 1:] - styled_acceleration[:, :-1]\n",
    "    original_jerk = original_acceleration[:, 1:] - original_acceleration[:, :-1]\n",
    "    \n",
    "    jerk_magnitude = tf.reduce_mean(tf.square(styled_jerk))\n",
    "    original_jerk_magnitude = tf.reduce_mean(tf.square(original_jerk))\n",
    "    \n",
    "    # Strong penalty for excessive jerk\n",
    "    losses['jerk_smoothness'] = tf.maximum(0.0, jerk_magnitude - original_jerk_magnitude * 1.1)\n",
    "    \n",
    "    # 4. High-frequency penalty - penalise rapid oscillations\n",
    "    def high_frequency_penalty(motion):\n",
    "        # Calculate second-order differences to detect high-frequency changes\n",
    "        diff1 = motion[:, 1:] - motion[:, :-1]\n",
    "        diff2 = diff1[:, 1:] - diff1[:, :-1]\n",
    "        diff3 = diff2[:, 1:] - diff2[:, :-1]\n",
    "        \n",
    "        sign_changes = tf.reduce_mean(tf.abs(tf.sign(diff3[:, 1:]) - tf.sign(diff3[:, :-1])))\n",
    "        return sign_changes\n",
    "    \n",
    "    styled_hf_penalty = high_frequency_penalty(styled_motion)\n",
    "    original_hf_penalty = high_frequency_penalty(original_robot_motion)\n",
    "    losses['high_frequency_penalty'] = tf.maximum(0.0, styled_hf_penalty - original_hf_penalty * 1.1)\n",
    "    \n",
    "    # Standard motion quality losses \n",
    "    motion_diff = styled_motion[:, 1:] - styled_motion[:, :-1]\n",
    "    original_diff = original_robot_motion[:, 1:] - original_robot_motion[:, :-1]\n",
    "    \n",
    "    smoothness_loss = tf.reduce_mean(tf.square(motion_diff))\n",
    "    original_smoothness = tf.reduce_mean(tf.square(original_diff))\n",
    "    losses['smoothness'] = tf.maximum(0.0, smoothness_loss - original_smoothness * 2.0)\n",
    "    \n",
    "    # Position constraint (allow deviation but prevent extreme jumps)\n",
    "    position_deviation = tf.reduce_mean(tf.square(styled_motion - original_robot_motion))\n",
    "    losses['position_constraint'] = position_deviation\n",
    "    \n",
    "    # Residual penalties (encourage smooth residuals)\n",
    "    losses['residual_penalty'] = tf.reduce_mean(tf.square(style_residual))\n",
    "    \n",
    "    # Residual smoothness penalty\n",
    "    residual_velocity = style_residual[:, 1:] - style_residual[:, :-1]\n",
    "    residual_acceleration = residual_velocity[:, 1:] - residual_velocity[:, :-1]\n",
    "    losses['residual_smoothness'] = tf.reduce_mean(tf.square(residual_acceleration))\n",
    "    \n",
    "    # Minimum change requirement \n",
    "    motion_change = tf.reduce_mean(tf.abs(styled_motion - original_robot_motion))\n",
    "    losses['minimum_change'] = tf.maximum(0.0, 0.15 - motion_change)  # Slightly reduced to balance with smoothness\n",
    "    \n",
    "    # Style differentiation\n",
    "    if style_labels is not None and len(set(style_labels)) > 1:\n",
    "        batch_size = tf.shape(styled_motion)[0]\n",
    "        diff_sum = 0.0\n",
    "        count = 0\n",
    "        \n",
    "        for i in range(min(3, batch_size)):\n",
    "            for j in range(i+1, min(3, batch_size)):\n",
    "                motion_distance = tf.reduce_mean(tf.square(styled_motion[i] - styled_motion[j]))\n",
    "                diff_sum += tf.maximum(0.0, 0.1 - motion_distance)\n",
    "                count += 1\n",
    "        \n",
    "        losses['style_differentiation'] = diff_sum / tf.maximum(1.0, tf.cast(count, tf.float32))\n",
    "        losses['style_consistency'] = 0.0\n",
    "    else:\n",
    "        losses['style_consistency'] = 0.0\n",
    "        losses['style_differentiation'] = 0.0\n",
    "    \n",
    "    total_loss = (\n",
    "        0.1 * losses['adversarial'] +                    # Reduced\n",
    "        1.0 * losses['velocity_smoothness'] +            # Velocity smoothness\n",
    "        3.0 * losses['acceleration_smoothness'] +        # Acceleration smoothness (key for jitter)\n",
    "        5.0 * losses['jerk_smoothness'] +                # Jerk penalty \n",
    "        2.0 * losses['high_frequency_penalty'] +         # High-frequency penalty\n",
    "        1.0 * losses['residual_smoothness'] +            # Smooth residuals\n",
    "        0.001 * losses['smoothness'] +                   # Smoothness \n",
    "        0.005 * losses['residual_penalty'] +             \n",
    "        10.0 * losses['min_variance_loss'] +             # Keep temporal variation\n",
    "        4.0 * losses['rhythm_deviation'] +               # Keep rhythm changes\n",
    "        2.0 * losses['rhythm_style_correlation'] +       # Keep style correlation\n",
    "        3.0 * losses['minimum_change'] +                 # Force style transfer\n",
    "        0.02 * losses['position_constraint'] +           \n",
    "        0.1 * losses['style_consistency'] +\n",
    "        0.3 * losses['style_differentiation']\n",
    "    )\n",
    "    \n",
    "    losses['total'] = total_loss\n",
    "    return losses\n",
    "\n",
    "def compute_discriminator_loss(discriminator, real_motions, fake_motions):\n",
    "    real_scores = discriminator(real_motions)\n",
    "    fake_scores = discriminator(fake_motions)\n",
    "    real_loss = tf.reduce_mean(tf.square(real_scores - 1.0))\n",
    "    fake_loss = tf.reduce_mean(tf.square(fake_scores))\n",
    "    return (real_loss + fake_loss) / 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83439fc",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f992527",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_video_and_poses(video_path, pose_path, target_size=(64, 64), sequence_length=128):\n",
    "    try:\n",
    "        video_frames = load_video(video_path, target_size, sequence_length)\n",
    "        poses = np.load(pose_path).astype(np.float32)\n",
    "        \n",
    "        if len(video_frames) != len(poses):\n",
    "            min_length = min(len(video_frames), len(poses))\n",
    "            video_frames = video_frames[:min_length]\n",
    "            poses = poses[:min_length]\n",
    "        \n",
    "        return video_frames.astype(np.float32), poses.astype(np.float32)\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "def organize_human_videos_with_poses():\n",
    "    mapping_path = \"robot_motion_data/preprocessed_poses/video_pose_mapping.pkl\"\n",
    "    \n",
    "    if not os.path.exists(mapping_path):\n",
    "        return {}\n",
    "    \n",
    "    with open(mapping_path, 'rb') as f:\n",
    "        mapping = pickle.load(f)\n",
    "    \n",
    "    validated_mapping = {}\n",
    "    for style, pairs in mapping.items():\n",
    "        validated_pairs = []\n",
    "        for video_path, pose_path in pairs:\n",
    "            if os.path.exists(video_path) and os.path.exists(pose_path):\n",
    "                validated_pairs.append((video_path, pose_path))\n",
    "        if validated_pairs:\n",
    "            validated_mapping[style] = validated_pairs\n",
    "    \n",
    "    return validated_mapping\n",
    "\n",
    "def create_mixed_style_task(robot_sequences, video_pose_mapping):\n",
    "    available_styles = list(video_pose_mapping.keys())\n",
    "    if len(available_styles) < 2:\n",
    "        # If only one style, create single-style task\n",
    "        style = available_styles[0]\n",
    "        style_pairs = video_pose_mapping[style]\n",
    "        \n",
    "        task_data = {'videos': [], 'poses': [], 'robot_motions': [], 'style_labels': []}\n",
    "        \n",
    "        for _ in range(3):  # 3 examples from same style\n",
    "            video_path, pose_path = style_pairs[np.random.randint(len(style_pairs))]\n",
    "            video_frames, poses = load_video_and_poses(video_path, pose_path)\n",
    "            if video_frames is not None and poses is not None:\n",
    "                robot_motion = robot_sequences[np.random.randint(len(robot_sequences))]\n",
    "                \n",
    "                task_data['videos'].append(video_frames)\n",
    "                task_data['poses'].append(poses)\n",
    "                task_data['robot_motions'].append(robot_motion)\n",
    "                task_data['style_labels'].append(style)\n",
    "    else:\n",
    "        # Mixed style task\n",
    "        task_data = {'videos': [], 'poses': [], 'robot_motions': [], 'style_labels': []}\n",
    "        \n",
    "        # Get one example from 3 different styles\n",
    "        selected_styles = np.random.choice(available_styles, size=min(3, len(available_styles)), replace=False)\n",
    "        \n",
    "        for style in selected_styles:\n",
    "            style_pairs = video_pose_mapping[style]\n",
    "            video_path, pose_path = style_pairs[np.random.randint(len(style_pairs))]\n",
    "            video_frames, poses = load_video_and_poses(video_path, pose_path)\n",
    "            if video_frames is not None and poses is not None:\n",
    "                robot_motion = robot_sequences[np.random.randint(len(robot_sequences))]\n",
    "                \n",
    "                task_data['videos'].append(video_frames)\n",
    "                task_data['poses'].append(poses)\n",
    "                task_data['robot_motions'].append(robot_motion)\n",
    "                task_data['style_labels'].append(style)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    if task_data['videos']:\n",
    "        task_data['videos'] = np.array(task_data['videos'], dtype=np.float32)\n",
    "        task_data['poses'] = np.array(task_data['poses'], dtype=np.float32)\n",
    "        task_data['robot_motions'] = np.array(task_data['robot_motions'], dtype=np.float32)\n",
    "    \n",
    "    return task_data\n",
    "\n",
    "def train_temporal_style_model(num_epochs=50, use_cross_attention=True):\n",
    "    \n",
    "    # Load data\n",
    "    robot_sequences = np.load(\"processed_robot_sequences.npy\").astype(np.float32)\n",
    "    video_pose_mapping = organize_human_videos_with_poses()\n",
    "    \n",
    "    if not video_pose_mapping:\n",
    "        print(\"No video-pose mapping found! Run preprocess_videos() first.\")\n",
    "        return None\n",
    "    \n",
    "    # Build model with configurable attention\n",
    "    model, discriminator, components = build_complete_temporal_model(use_cross_attention=use_cross_attention)\n",
    "    \n",
    "    attention_type = \"with cross-modal attention\" if use_cross_attention else \"without cross-modal attention\"\n",
    "    print(f\"Building temporal style transfer model {attention_type}\")\n",
    "    \n",
    "    # Optimizers\n",
    "    gen_optimizer = tf.keras.optimizers.Adam(0.0003)\n",
    "    disc_optimizer = tf.keras.optimizers.Adam(0.0003)\n",
    "    \n",
    "    print(f\"Training temporal style transfer model with {len(robot_sequences)} robot sequences\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_gen_loss = 0.0\n",
    "        epoch_disc_loss = 0.0\n",
    "        successful_tasks = 0\n",
    "        \n",
    "        for task_num in range(10):  # More tasks per epoch\n",
    "            try:\n",
    "                task = create_mixed_style_task(robot_sequences, video_pose_mapping)\n",
    "                \n",
    "                videos = tf.convert_to_tensor(task['videos'], dtype=tf.float32)\n",
    "                poses = tf.convert_to_tensor(task['poses'], dtype=tf.float32)\n",
    "                robot_motions = tf.convert_to_tensor(task['robot_motions'], dtype=tf.float32)\n",
    "                style_labels = task['style_labels']\n",
    "                \n",
    "                # Train generator\n",
    "                with tf.GradientTape() as gen_tape:\n",
    "                    outputs = model([videos, poses, robot_motions], training=True)\n",
    "                    losses = compute_enhanced_losses(outputs, robot_motions, outputs['style_sequence'], style_labels)\n",
    "                    gen_loss = losses['total']\n",
    "                \n",
    "                gen_grads = gen_tape.gradient(gen_loss, model.trainable_variables)\n",
    "                gen_optimizer.apply_gradients(zip(gen_grads, model.trainable_variables))\n",
    "                \n",
    "                # Train discriminator\n",
    "                with tf.GradientTape() as disc_tape:\n",
    "                    disc_loss = compute_discriminator_loss(discriminator, robot_motions, outputs['styled_motion'])\n",
    "                \n",
    "                disc_grads = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "                disc_optimizer.apply_gradients(zip(disc_grads, discriminator.trainable_variables))\n",
    "                \n",
    "                epoch_gen_loss += gen_loss\n",
    "                epoch_disc_loss += disc_loss\n",
    "                successful_tasks += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Task {task_num + 1} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if successful_tasks > 0:\n",
    "            avg_gen_loss = epoch_gen_loss / successful_tasks\n",
    "            avg_disc_loss = epoch_disc_loss / successful_tasks\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}: Gen Loss: {avg_gen_loss:.4f}, Disc Loss: {avg_disc_loss:.4f}\")\n",
    "    \n",
    "    # Save model with attention type in filename\n",
    "    attention_suffix = \"_cross_attention\" if use_cross_attention else \"_simple\"\n",
    "    model.save_weights(f\"temporal_style_transfer_model{attention_suffix}.weights.h5\")\n",
    "    components['temporal_style_encoder'].save_weights(f\"temporal_style_encoder{attention_suffix}.weights.h5\")\n",
    "    components['robot_encoder'].save_weights(f\"robot_encoder{attention_suffix}.weights.h5\")\n",
    "    components['transformer'].save_weights(f\"temporal_transformer{attention_suffix}.weights.h5\")\n",
    "    \n",
    "    print(f\"Temporal style transfer training completed {attention_type}!\")\n",
    "    return model, components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6555e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_temporal_style_model(num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b0a59b",
   "metadata": {},
   "source": [
    "## Few shot adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0eb9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adapt_to_style_temporal(style_name, adaptation_steps=5, use_cross_attention=True):    \n",
    "    model, discriminator, components = build_complete_temporal_model(use_cross_attention=use_cross_attention)\n",
    "    \n",
    "    attention_suffix = \"_cross_attention\" if use_cross_attention else \"_simple\"\n",
    "    \n",
    "    try:\n",
    "        components['temporal_style_encoder'].load_weights(f\"temporal_style_encoder{attention_suffix}.weights.h5\")\n",
    "        components['robot_encoder'].load_weights(f\"robot_encoder{attention_suffix}.weights.h5\")\n",
    "        components['transformer'].load_weights(f\"temporal_transformer{attention_suffix}.weights.h5\")\n",
    "        \n",
    "        attention_type = \"with cross-modal attention\" if use_cross_attention else \"without cross-modal attention\"\n",
    "        print(f\"Loaded temporal model weights {attention_type} for adaptation to {style_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"No trained model found: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Load data\n",
    "    robot_sequences = np.load(\"processed_robot_sequences.npy\")\n",
    "    video_pose_mapping = organize_human_videos_with_poses()\n",
    "    \n",
    "    if style_name not in video_pose_mapping:\n",
    "        print(f\"Style '{style_name}' not found!\")\n",
    "        return None\n",
    "    \n",
    "    # Load style examples\n",
    "    style_pairs = video_pose_mapping[style_name]\n",
    "    videos = []\n",
    "    poses = []\n",
    "    \n",
    "    for video_path, pose_path in style_pairs[:6]:\n",
    "        try:\n",
    "            video_frames, pose_data = load_video_and_poses(video_path, pose_path)\n",
    "            if video_frames is not None and pose_data is not None:\n",
    "                videos.append(video_frames)\n",
    "                poses.append(pose_data)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if not videos:\n",
    "        print(\"No valid examples found\")\n",
    "        return None\n",
    "    \n",
    "    videos = np.array(videos, dtype=np.float32)\n",
    "    poses = np.array(poses, dtype=np.float32)\n",
    "    motions = robot_sequences[:len(videos)]\n",
    "    \n",
    "    # Adaptation optimizer (lower learning rate)\n",
    "    optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "    \n",
    "    for step in range(adaptation_steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = model([videos, poses, motions], training=True)\n",
    "            style_labels = [style_name] * len(videos)\n",
    "            \n",
    "            # Use enhanced loss function for adaptation\n",
    "            losses = compute_enhanced_losses(outputs, motions, outputs['style_sequence'], style_labels)\n",
    "            \n",
    "            # Focus on style-specific losses during adaptation\n",
    "            adaptation_loss = (\n",
    "                5.0 * losses['min_variance_loss'] +           # Force temporal variation\n",
    "                3.0 * losses['rhythm_deviation'] +            # Encourage rhythm changes\n",
    "                2.0 * losses['rhythm_style_correlation'] +    # Link rhythm to style\n",
    "                1.0 * losses['style_differentiation'] +       # Differentiate from other styles\n",
    "                0.5 * losses['jerk_smoothness'] +             # Prevent excessive jerkiness\n",
    "                0.2 * losses['adversarial'] +                # Slight adversarial pressure\n",
    "                0.1 * losses['residual_penalty'] +           # Minor residual penalty\n",
    "                0.05 * losses['position_constraint']         # Minimal position constraint\n",
    "            )\n",
    "        \n",
    "        gradients = tape.gradient(adaptation_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        print(f\"Adaptation step {step + 1}: Loss {adaptation_loss:.4f}\")\n",
    "    \n",
    "    # Save adapted model\n",
    "    components['temporal_style_encoder'].save_weights(f\"adapted_{style_name}_temporal_encoder{attention_suffix}.weights.h5\")\n",
    "    components['robot_encoder'].save_weights(f\"adapted_{style_name}_robot_encoder{attention_suffix}.weights.h5\")\n",
    "    components['transformer'].save_weights(f\"adapted_{style_name}_temporal_transformer{attention_suffix}.weights.h5\")\n",
    "    \n",
    "    attention_type = \"with cross-modal attention\" if use_cross_attention else \"without cross-modal attention\"\n",
    "    print(f\"Temporal model adaptation to '{style_name}' completed {attention_type}!\")\n",
    "    return model, components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f61d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_to_style_temporal(\"Flamenco\", adaptation_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d7e4ef",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ba09f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reference_poses(video_path):\n",
    "    try:\n",
    "        mapping_path = \"robot_motion_data/preprocessed_poses/video_pose_mapping.pkl\"\n",
    "        if not os.path.exists(mapping_path):\n",
    "            return None\n",
    "        \n",
    "        with open(mapping_path, 'rb') as f:\n",
    "            mapping = pickle.load(f)\n",
    "        \n",
    "        for style, pairs in mapping.items():\n",
    "            for vid_path, pose_path in pairs:\n",
    "                if os.path.abspath(vid_path) == os.path.abspath(video_path):\n",
    "                    poses = np.load(pose_path).astype(np.float32)\n",
    "                    return poses\n",
    "        \n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def generate_temporal_styled_motion(robot_csv_path, reference_video_path, style_name=None, creativity=1.0, use_cross_attention=True):\n",
    "    \n",
    "    # Load model components\n",
    "    model, discriminator, components = build_complete_temporal_model(use_cross_attention=use_cross_attention)\n",
    "    \n",
    "    attention_suffix = \"_cross_attention\" if use_cross_attention else \"_simple\"\n",
    "    \n",
    "    # Load weights (try adapted version first if style_name provided)\n",
    "    weight_files = []\n",
    "    if style_name:\n",
    "        weight_files.append((\n",
    "            f\"adapted_{style_name}_temporal_encoder{attention_suffix}.weights.h5\",\n",
    "            f\"adapted_{style_name}_robot_encoder{attention_suffix}.weights.h5\",\n",
    "            f\"adapted_{style_name}_temporal_transformer{attention_suffix}.weights.h5\"\n",
    "        ))\n",
    "    \n",
    "    # Fallback to general trained model\n",
    "    weight_files.append((\n",
    "        f\"temporal_style_encoder{attention_suffix}.weights.h5\",\n",
    "        f\"robot_encoder{attention_suffix}.weights.h5\",\n",
    "        f\"temporal_transformer{attention_suffix}.weights.h5\"\n",
    "    ))\n",
    "    \n",
    "    loaded = False\n",
    "    for encoder_weights, robot_weights, transformer_weights in weight_files:\n",
    "        try:\n",
    "            components['temporal_style_encoder'].load_weights(encoder_weights)\n",
    "            components['robot_encoder'].load_weights(robot_weights)\n",
    "            components['transformer'].load_weights(transformer_weights)\n",
    "            loaded = True\n",
    "            attention_type = \"with cross-modal attention\" if use_cross_attention else \"without cross-modal attention\"\n",
    "            print(f\"Loaded weights {attention_type}: {transformer_weights}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    if not loaded:\n",
    "        print(\"No trained weights found\")\n",
    "        return None\n",
    "    \n",
    "    # Load data\n",
    "    with open(\"robot_scaler.pkl\", 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    \n",
    "    robot_data = load_robot_csv(robot_csv_path)\n",
    "    reference_video = load_video(reference_video_path)\n",
    "    reference_poses = load_reference_poses(reference_video_path)\n",
    "    \n",
    "    if reference_poses is None:\n",
    "        print(\"Could not load poses for reference video\")\n",
    "        return None\n",
    "    \n",
    "    # Scale robot data\n",
    "    scaled_robot_data = scaler.transform(robot_data).astype(np.float32)\n",
    "    \n",
    "    # Extract temporal style features (now with optional cross-modal attention)\n",
    "    video_tensor = tf.convert_to_tensor(np.expand_dims(reference_video, 0), dtype=tf.float32)\n",
    "    pose_tensor = tf.convert_to_tensor(np.expand_dims(reference_poses, 0), dtype=tf.float32)\n",
    "    style_sequence = components['temporal_style_encoder']([video_tensor, pose_tensor])[0]\n",
    "    \n",
    "    print(f\"Robot data shape: {scaled_robot_data.shape}\")\n",
    "    print(f\"Style sequence shape: {style_sequence.shape}\")\n",
    "    attention_type = \"with cross-modal attention\" if use_cross_attention else \"without cross-modal attention\"\n",
    "    print(f\"Using temporal style encoder {attention_type}\")\n",
    "    \n",
    "    # Use overlapping windows with enhanced smooth blending\n",
    "    sequence_length = 128\n",
    "    window_step = 32  # Increased overlap (75% overlap instead of 50%)\n",
    "    num_timesteps = len(scaled_robot_data)\n",
    "    \n",
    "    # Initialize styled motion with better interpolation\n",
    "    styled_motion = np.zeros_like(scaled_robot_data)\n",
    "    weight_accumulator = np.zeros(num_timesteps)\n",
    "    \n",
    "    for window_start in range(0, num_timesteps - sequence_length + 1, window_step):\n",
    "        window_end = window_start + sequence_length\n",
    "        \n",
    "        # Prepare robot window\n",
    "        robot_window = scaled_robot_data[window_start:window_end]\n",
    "        \n",
    "        # Prepare style window (cycle through style if needed)\n",
    "        style_window = np.zeros((sequence_length, style_sequence.shape[-1]), dtype=np.float32)\n",
    "        for i in range(sequence_length):\n",
    "            style_idx = (window_start + i) % len(style_sequence)\n",
    "            style_window[i] = style_sequence[style_idx]\n",
    "        \n",
    "        # Generate temporal residuals\n",
    "        robot_tensor = tf.convert_to_tensor(np.expand_dims(robot_window, 0), dtype=tf.float32)\n",
    "        style_tensor = tf.convert_to_tensor(np.expand_dims(style_window, 0), dtype=tf.float32)\n",
    "        \n",
    "        try:\n",
    "            residuals = components['transformer']([robot_tensor, style_tensor])[0]\n",
    "            \n",
    "            # Convert TensorFlow tensor to numpy array\n",
    "            residuals_np = residuals.numpy()\n",
    "            \n",
    "            # Debug information\n",
    "            if window_start == 0:  # Print only for first window to avoid spam\n",
    "                print(f\"Residuals shape: {residuals_np.shape}, type: {type(residuals_np)}\")\n",
    "                print(f\"Robot window shape: {robot_window.shape}, type: {type(robot_window)}\")\n",
    "            \n",
    "            # Apply creativity scaling\n",
    "            scaled_residuals = creativity * residuals_np\n",
    "            \n",
    "            # Create styled motion for this window\n",
    "            styled_window = robot_window + scaled_residuals\n",
    "            \n",
    "            # Create enhanced blending weights (smoother transitions)\n",
    "            blend_weights = np.ones(sequence_length)\n",
    "            fade_length = 64  # Longer fade regions (50% of window)\n",
    "            \n",
    "            # Cosine fade-in at start (smoother than linear)\n",
    "            fade_in = 0.5 * (1 - np.cos(np.pi * np.linspace(0, 1, fade_length)))\n",
    "            blend_weights[:fade_length] = 0.1 + 0.9 * fade_in\n",
    "            \n",
    "            # Cosine fade-out at end\n",
    "            fade_out = 0.5 * (1 + np.cos(np.pi * np.linspace(0, 1, fade_length)))\n",
    "            blend_weights[-fade_length:] = 0.1 + 0.9 * fade_out\n",
    "            \n",
    "            # Apply temporal smoothing to the styled window itself\n",
    "            for joint in range(7):\n",
    "                joint_data = styled_window[:, joint]\n",
    "                \n",
    "                # Light smoothing with preserving boundaries\n",
    "                if len(joint_data) >= 5:\n",
    "                    # Use simple moving average in the middle, preserve edges\n",
    "                    smoothed = joint_data.copy()  # Now joint_data is numpy array\n",
    "                    for i in range(2, len(joint_data) - 2):\n",
    "                        smoothed[i] = 0.2 * (joint_data[i-2] + joint_data[i-1] + joint_data[i] + \n",
    "                                           joint_data[i+1] + joint_data[i+2])\n",
    "                    \n",
    "                    # Blend original and smoothed (favor original for style preservation)\n",
    "                    styled_window[:, joint] = 0.7 * joint_data + 0.3 * smoothed\n",
    "            \n",
    "            # Accumulate weighted results\n",
    "            for i, global_idx in enumerate(range(window_start, window_end)):\n",
    "                if global_idx < num_timesteps:\n",
    "                    weight = blend_weights[i]\n",
    "                    styled_motion[global_idx] += weight * styled_window[i]\n",
    "                    weight_accumulator[global_idx] += weight\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing window {window_start}-{window_end}: {e}\")\n",
    "            print(f\"Error type: {type(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # Handle remaining timesteps at the end\n",
    "    if window_start + sequence_length < num_timesteps:\n",
    "        remaining_start = num_timesteps - sequence_length\n",
    "        remaining_robot = scaled_robot_data[remaining_start:]\n",
    "        \n",
    "        # Pad to 128 length\n",
    "        robot_window = np.zeros((sequence_length, 7), dtype=np.float32)\n",
    "        robot_window[:len(remaining_robot)] = remaining_robot\n",
    "        robot_window[len(remaining_robot):] = remaining_robot[-1:]\n",
    "        \n",
    "        # Style window\n",
    "        style_window = np.zeros((sequence_length, style_sequence.shape[-1]), dtype=np.float32)\n",
    "        for i in range(sequence_length):\n",
    "            style_idx = (remaining_start + i) % len(style_sequence)\n",
    "            style_window[i] = style_sequence[style_idx]\n",
    "        \n",
    "        try:\n",
    "            robot_tensor = tf.convert_to_tensor(np.expand_dims(robot_window, 0), dtype=tf.float32)\n",
    "            style_tensor = tf.convert_to_tensor(np.expand_dims(style_window, 0), dtype=tf.float32)\n",
    "            \n",
    "            residuals = components['transformer']([robot_tensor, style_tensor])[0]\n",
    "            \n",
    "            # Convert TensorFlow tensor to numpy array\n",
    "            residuals_np = residuals.numpy()\n",
    "            scaled_residuals = creativity * residuals_np[:len(remaining_robot)]\n",
    "            \n",
    "            styled_remaining = remaining_robot + scaled_residuals\n",
    "            \n",
    "            # Add remaining part\n",
    "            for i, global_idx in enumerate(range(remaining_start, num_timesteps)):\n",
    "                if weight_accumulator[global_idx] == 0:\n",
    "                    styled_motion[global_idx] = styled_remaining[i]\n",
    "                    weight_accumulator[global_idx] = 1.0\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing remaining timesteps: {e}\")\n",
    "            # Fill remaining with original data\n",
    "            for i, global_idx in enumerate(range(remaining_start, num_timesteps)):\n",
    "                if weight_accumulator[global_idx] == 0:\n",
    "                    styled_motion[global_idx] = remaining_robot[i]\n",
    "                    weight_accumulator[global_idx] = 1.0\n",
    "    \n",
    "    # Normalize by accumulated weights and apply final light smoothing\n",
    "    for i in range(num_timesteps):\n",
    "        if weight_accumulator[i] > 0:\n",
    "            styled_motion[i] /= weight_accumulator[i]\n",
    "        else:\n",
    "            styled_motion[i] = scaled_robot_data[i]  # Fallback to original\n",
    "    \n",
    "    # Final light smoothing to ensure continuity between blended regions\n",
    "    for joint in range(7):\n",
    "        joint_motion = styled_motion[:, joint]\n",
    "        if len(joint_motion) >= 3:\n",
    "            # Very light 3-point smoothing\n",
    "            smoothed = joint_motion.copy()\n",
    "            for i in range(1, len(joint_motion) - 1):\n",
    "                smoothed[i] = 0.25 * joint_motion[i-1] + 0.5 * joint_motion[i] + 0.25 * joint_motion[i+1]\n",
    "            \n",
    "            # Minimal blending (95% original to preserve style)\n",
    "            styled_motion[:, joint] = 0.95 * joint_motion + 0.05 * smoothed\n",
    "    \n",
    "    # Convert back to original scale\n",
    "    final_motion = scaler.inverse_transform(styled_motion).astype(np.float32)\n",
    "    \n",
    "    # Save result\n",
    "    base_name = os.path.splitext(os.path.basename(robot_csv_path))[0]\n",
    "    style_suffix = f\"_{style_name}\" if style_name else \"\"\n",
    "    attention_suffix_file = \"_cross_attn\" if use_cross_attention else \"_simple\"\n",
    "    creativity_suffix = f\"_c{creativity}_temporal{attention_suffix_file}\"\n",
    "    output_path = f\"temporal_styled{style_suffix}{creativity_suffix}_{base_name}.csv\"\n",
    "    \n",
    "    np.savetxt(output_path, final_motion, delimiter=',',\n",
    "               header='Position1,Position2,Position3,Position4,Position5,Position6,Position7',\n",
    "               comments='')\n",
    "    \n",
    "    print(f\"Generated temporal styled motion saved to: {output_path}\")\n",
    "    \n",
    "    \n",
    "    # Save result\n",
    "    base_name = os.path.splitext(os.path.basename(robot_csv_path))[0]\n",
    "    style_suffix = f\"_{style_name}\" if style_name else \"\"\n",
    "    creativity_suffix = f\"_c{creativity}_temporal_smooth\"\n",
    "    output_path = f\"temporal_styled{style_suffix}{creativity_suffix}_{base_name}.csv\"\n",
    "    \n",
    "    np.savetxt(output_path, final_motion, delimiter=',',\n",
    "               header='Position1,Position2,Position3,Position4,Position5,Position6,Position7',\n",
    "               comments='')\n",
    "    \n",
    "    return final_motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d966ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_temporal_styled_motion(\n",
    "    \"robot_motion_data/robot_motions/example.csv\", \n",
    "    \"robot_motion_data/human_videos/Flamenco/example.mp4\", \n",
    "    \"Flamenco\", \n",
    "    creativity=0.25, \n",
    "    use_cross_attention=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92783415",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0ab752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_style_transfer_comprehensive(original_csv_path, styled_csv_path, reference_video_path=None):\n",
    "    \n",
    "    print(f\"Evaluating style transfer comprehensively:\")\n",
    "    print(f\"  Original: {os.path.basename(original_csv_path)}\")\n",
    "    print(f\"  Styled: {os.path.basename(styled_csv_path)}\")\n",
    "    \n",
    "    # Load robot motions\n",
    "    try:\n",
    "        original_robot = np.loadtxt(original_csv_path, delimiter=',', skiprows=1)\n",
    "        styled_robot = np.loadtxt(styled_csv_path, delimiter=',', skiprows=1)\n",
    "        \n",
    "        print(f\"Loaded original robot motion: {original_robot.shape}\")\n",
    "        print(f\"Loaded styled robot motion: {styled_robot.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV files: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Ensure same dimensions\n",
    "    min_length = min(len(original_robot), len(styled_robot))\n",
    "    min_joints = min(original_robot.shape[1], styled_robot.shape[1])\n",
    "    \n",
    "    original_robot = original_robot[:min_length, :min_joints]\n",
    "    styled_robot = styled_robot[:min_length, :min_joints]\n",
    "    \n",
    "    print(f\"Analyzing {min_length} timesteps across {min_joints} joints\")\n",
    "    print(f\"Final shapes - Original: {original_robot.shape}, Styled: {styled_robot.shape}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. IMPROVED TRAJECTORY PRESERVATION\n",
    "    trajectory_results = evaluate_trajectory_preservation_improved(original_robot, styled_robot)\n",
    "    results['trajectory_analysis'] = trajectory_results\n",
    "    \n",
    "    # 2. PHYSICAL DEVIATION (from original evaluation)\n",
    "    joint_deviations = np.std(styled_robot - original_robot, axis=0)\n",
    "    avg_deviation = np.mean(joint_deviations)\n",
    "    max_deviation = np.max(joint_deviations)\n",
    "    \n",
    "    results['physical_deviation'] = {\n",
    "        'average': avg_deviation,\n",
    "        'maximum': max_deviation,\n",
    "        'per_joint': joint_deviations\n",
    "    }\n",
    "    \n",
    "    # 3. MOTION CHARACTERISTICS ANALYSIS\n",
    "    original_characteristics = analyze_motion_characteristics(original_robot)\n",
    "    styled_characteristics = analyze_motion_characteristics(styled_robot)\n",
    "    \n",
    "    results['motion_profiles'] = {\n",
    "        'original': original_characteristics,\n",
    "        'styled': styled_characteristics\n",
    "    }\n",
    "    \n",
    "    # 4. STYLE DIFFERENTIATION\n",
    "    speed_change = abs(styled_characteristics['avg_speed'] - original_characteristics['avg_speed']) / max(original_characteristics['avg_speed'], 1e-6)\n",
    "    accel_change = abs(styled_characteristics['avg_acceleration'] - original_characteristics['avg_acceleration']) / max(original_characteristics['avg_acceleration'], 1e-6)\n",
    "    coordination_change = abs(styled_characteristics['coordination'] - original_characteristics['coordination'])\n",
    "    range_change = abs(styled_characteristics['movement_range'] - original_characteristics['movement_range']) / max(original_characteristics['movement_range'], 1e-6)\n",
    "    \n",
    "    # Overall style change\n",
    "    style_change_score = np.mean([speed_change, accel_change, coordination_change, range_change])\n",
    "    \n",
    "    results['style_characteristics'] = {\n",
    "        'speed_change': speed_change,\n",
    "        'acceleration_change': accel_change,\n",
    "        'coordination_change': coordination_change,\n",
    "        'range_change': range_change,\n",
    "        'overall_style_change': style_change_score\n",
    "    }\n",
    "    \n",
    "    # 5. SMOOTHNESS EVALUATION\n",
    "    original_smoothness = compute_smoothness(original_robot)\n",
    "    styled_smoothness = compute_smoothness(styled_robot)\n",
    "    smoothness_ratio = styled_smoothness / max(original_smoothness, 1e-6)\n",
    "    \n",
    "    results['motion_quality'] = {\n",
    "        'original_smoothness': original_smoothness,\n",
    "        'styled_smoothness': styled_smoothness,\n",
    "        'smoothness_ratio': smoothness_ratio\n",
    "    }\n",
    "    \n",
    "    # 6. FREQUENCY ANALYSIS\n",
    "    original_freq = analyze_frequency_content(original_robot)\n",
    "    styled_freq = analyze_frequency_content(styled_robot)\n",
    "    frequency_change = abs(styled_freq - original_freq) / max(original_freq, 1e-6)\n",
    "    \n",
    "    results['frequency_analysis'] = {\n",
    "        'original_freq': original_freq,\n",
    "        'styled_freq': styled_freq,\n",
    "        'frequency_change': frequency_change\n",
    "    }\n",
    "    \n",
    "    # 7.  OVERALL ASSESSMENT\n",
    "    trajectory_score = trajectory_results['functional_preservation']['overall_score']\n",
    "    deviation_score = max(0, 1 - avg_deviation / 2.0)\n",
    "    style_score = min(1, style_change_score)\n",
    "    \n",
    "    overall_quality = (\n",
    "        trajectory_score * 0.6 +    \n",
    "        deviation_score * 0.2 +     \n",
    "        style_score * 0.2           \n",
    "    )\n",
    "    \n",
    "    results['overall_assessment'] = {\n",
    "        'overall_quality': overall_quality,\n",
    "        'component_scores': {\n",
    "            'trajectory_score': trajectory_score,\n",
    "            'deviation_score': deviation_score,\n",
    "            'style_score': style_score\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_temporal_dynamics(motion_sequence):\n",
    "    # Detect rhythm patterns\n",
    "    fft = np.fft.fft(motion_sequence)\n",
    "    power_spectrum = np.abs(fft[:len(fft)//2])\n",
    "    dominant_frequency = np.argmax(power_spectrum)\n",
    "    rhythm_strength = np.max(power_spectrum) / np.mean(power_spectrum)\n",
    "    \n",
    "    # Analyze motion phases (acceleration vs deceleration)\n",
    "    velocity = np.diff(motion_sequence)\n",
    "    acceleration = np.diff(velocity)\n",
    "    \n",
    "    return {\n",
    "        'dominant_frequency': dominant_frequency,\n",
    "        'rhythm_strength': rhythm_strength,\n",
    "        'phase_transitions': len(np.where(np.diff(np.sign(acceleration)))[0]),\n",
    "        'motion_complexity': np.std(acceleration)\n",
    "    }\n",
    "def extract_video_style_features(video_path):\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        \n",
    "        # Read video frames\n",
    "        while len(frames) < 128:  # Match training sequence length\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.resize(frame, (64, 64))\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = frame.astype(np.float32) / 255.0\n",
    "            frames.append(frame)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        # Pad if necessary\n",
    "        while len(frames) < 128:\n",
    "            frames.append(frames[-1])\n",
    "        \n",
    "        video_array = np.array(frames[:128])\n",
    "        \n",
    "        # Extract motion features (frame differences)\n",
    "        motion_vectors = []\n",
    "        for t in range(1, len(video_array)):\n",
    "            motion = video_array[t] - video_array[t-1]\n",
    "            motion_magnitude = np.mean(np.abs(motion))\n",
    "            motion_vectors.append(motion_magnitude)\n",
    "        \n",
    "        motion_vectors.insert(0, motion_vectors[0])  # Pad first frame\n",
    "        \n",
    "        return {\n",
    "            'motion_intensity': np.mean(motion_vectors),\n",
    "            'motion_variance': np.var(motion_vectors),\n",
    "            'motion_profile': motion_vectors,\n",
    "            'temporal_dynamics': analyze_temporal_dynamics(motion_vectors)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting video features: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_pose_style_features(pose_path):\n",
    "    try:\n",
    "        poses = np.load(pose_path).astype(np.float32)\n",
    "        \n",
    "        # Reshape to (time, joints, coordinates) assuming 33 joints with 4 coords each\n",
    "        poses_reshaped = poses.reshape(poses.shape[0], 33, 4)\n",
    "        pose_coords = poses_reshaped[:, :, :3]  # Only x, y, z coordinates\n",
    "        \n",
    "        # Calculate pose dynamics\n",
    "        pose_velocity = np.diff(pose_coords, axis=0)\n",
    "        pose_acceleration = np.diff(pose_velocity, axis=0)\n",
    "        \n",
    "        # Extract rhythm and coordination features\n",
    "        velocity_magnitude = np.mean(np.abs(pose_velocity), axis=(1, 2))\n",
    "        acceleration_magnitude = np.mean(np.abs(pose_acceleration), axis=(1, 2))\n",
    "        \n",
    "        # Joint coordination patterns (correlation between different joints)\n",
    "        coordination_features = []\n",
    "        for i in range(0, 33, 4):  # Sample every 4th joint\n",
    "            if i < pose_coords.shape[1]:\n",
    "                joint_motion = pose_coords[:, i, :]\n",
    "                joint_velocity = np.mean(np.abs(np.diff(joint_motion, axis=0)), axis=1)\n",
    "                coordination_features.append(np.mean(joint_velocity))\n",
    "        \n",
    "        return {\n",
    "            'rhythm_intensity': np.mean(velocity_magnitude),\n",
    "            'rhythm_variance': np.var(velocity_magnitude),\n",
    "            'acceleration_intensity': np.mean(acceleration_magnitude),\n",
    "            'acceleration_variance': np.var(acceleration_magnitude),\n",
    "            'coordination_score': np.mean(coordination_features),\n",
    "            'movement_range': np.mean(np.ptp(pose_coords, axis=0)),\n",
    "            'temporal_consistency': analyze_pose_temporal_patterns(pose_coords)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting pose features: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_robot_motion_style_features(robot_motion):    \n",
    "    # Calculate motion dynamics\n",
    "    velocity = np.diff(robot_motion, axis=0)\n",
    "    acceleration = np.diff(velocity, axis=0)\n",
    "    jerk = np.diff(acceleration, axis=0)\n",
    "    \n",
    "    # Speed and rhythm characteristics\n",
    "    speed = np.linalg.norm(velocity, axis=1)\n",
    "    accel_magnitude = np.linalg.norm(acceleration, axis=1)\n",
    "    \n",
    "    # Joint coordination\n",
    "    joint_correlations = []\n",
    "    for i in range(robot_motion.shape[1]):\n",
    "        for j in range(i+1, robot_motion.shape[1]):\n",
    "            try:\n",
    "                corr, _ = pearsonr(robot_motion[:, i], robot_motion[:, j])\n",
    "                if not np.isnan(corr):\n",
    "                    joint_correlations.append(abs(corr))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    coordination_score = np.mean(joint_correlations) if joint_correlations else 0\n",
    "    \n",
    "    # Movement range and spatial characteristics\n",
    "    movement_range = np.ptp(robot_motion, axis=0).mean()\n",
    "    \n",
    "    # Temporal dynamics (similar to video analysis)\n",
    "    temporal_dynamics = analyze_temporal_dynamics(speed)\n",
    "    \n",
    "    return {\n",
    "        'motion_intensity': np.mean(speed),\n",
    "        'motion_variance': np.var(speed),\n",
    "        'rhythm_intensity': np.mean(speed),\n",
    "        'rhythm_variance': np.var(speed),\n",
    "        'acceleration_intensity': np.mean(accel_magnitude),\n",
    "        'acceleration_variance': np.var(accel_magnitude),\n",
    "        'coordination_score': coordination_score,\n",
    "        'movement_range': movement_range,\n",
    "        'temporal_consistency': {\n",
    "            'movement_fluidity': 1.0 / (1.0 + np.std(speed)),\n",
    "            'acceleration_ratio': len(np.where(np.diff(speed) > 0)[0]) / max(1, len(speed)-1),\n",
    "            'spatial_coverage': np.std(robot_motion, axis=0).mean(),\n",
    "            'temporal_stability': 1.0 / (1.0 + np.std(np.diff(speed)))\n",
    "        },\n",
    "        'temporal_dynamics': temporal_dynamics\n",
    "    }\n",
    "\n",
    "def load_reference_style_data(style_name, video_pose_mapping_path=\"robot_motion_data/preprocessed_poses/video_pose_mapping.pkl\"):\n",
    "    try:\n",
    "        if not os.path.exists(video_pose_mapping_path):\n",
    "            print(f\"Warning: Video-pose mapping file not found: {video_pose_mapping_path}\")\n",
    "            return None, None\n",
    "        \n",
    "        with open(video_pose_mapping_path, 'rb') as f:\n",
    "            mapping = pickle.load(f)\n",
    "        \n",
    "        if style_name not in mapping:\n",
    "            print(f\"Warning: Style '{style_name}' not found in mapping. Available styles: {list(mapping.keys())}\")\n",
    "            return None, None\n",
    "        \n",
    "        style_pairs = mapping[style_name]\n",
    "        \n",
    "        # Analyse multiple examples if available\n",
    "        video_features_list = []\n",
    "        pose_features_list = []\n",
    "        \n",
    "        for video_path, pose_path in style_pairs[:3]:  \n",
    "            if os.path.exists(video_path) and os.path.exists(pose_path):\n",
    "                video_feat = extract_video_style_features(video_path)\n",
    "                pose_feat = extract_pose_style_features(pose_path)\n",
    "                \n",
    "                if video_feat:\n",
    "                    video_features_list.append(video_feat)\n",
    "                if pose_feat:\n",
    "                    pose_features_list.append(pose_feat)\n",
    "        \n",
    "        # Average features across examples\n",
    "        if video_features_list:\n",
    "            avg_video_features = {}\n",
    "            for key in video_features_list[0].keys():\n",
    "                if key != 'motion_profile': \n",
    "                    if isinstance(video_features_list[0][key], dict):\n",
    "                        avg_video_features[key] = {}\n",
    "                        for subkey in video_features_list[0][key].keys():\n",
    "                            values = [vf[key][subkey] for vf in video_features_list if subkey in vf[key]]\n",
    "                            avg_video_features[key][subkey] = np.mean(values) if values else 0\n",
    "                    else:\n",
    "                        values = [vf[key] for vf in video_features_list]\n",
    "                        avg_video_features[key] = np.mean(values)\n",
    "        else:\n",
    "            avg_video_features = None\n",
    "        \n",
    "        if pose_features_list:\n",
    "            avg_pose_features = {}\n",
    "            for key in pose_features_list[0].keys():\n",
    "                if isinstance(pose_features_list[0][key], dict):\n",
    "                    avg_pose_features[key] = {}\n",
    "                    for subkey in pose_features_list[0][key].keys():\n",
    "                        values = [pf[key][subkey] for pf in pose_features_list if subkey in pf[key]]\n",
    "                        avg_pose_features[key][subkey] = np.mean(values) if values else 0\n",
    "                else:\n",
    "                    values = [pf[key] for pf in pose_features_list]\n",
    "                    avg_pose_features[key] = np.mean(values)\n",
    "        else:\n",
    "            avg_pose_features = None\n",
    "        \n",
    "        return avg_video_features, avg_pose_features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading reference style data: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def compare_style_features(robot_features, video_features=None, pose_features=None, style_name=\"unknown\"):\n",
    "    \n",
    "    style_similarity_results = {\n",
    "        'style_name': style_name,\n",
    "        'video_similarity': {},\n",
    "        'pose_similarity': {},\n",
    "        'overall_style_match': 0.0\n",
    "    }\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    if video_features:\n",
    "        video_sim = {}\n",
    "        \n",
    "        # Motion intensity similarity\n",
    "        motion_sim = 1.0 - abs(robot_features['motion_intensity'] - video_features['motion_intensity']) / \\\n",
    "                     max(robot_features['motion_intensity'], video_features['motion_intensity'], 1e-6)\n",
    "        video_sim['motion_intensity'] = max(0, motion_sim)\n",
    "        \n",
    "        # Motion variance similarity\n",
    "        variance_sim = 1.0 - abs(robot_features['motion_variance'] - video_features['motion_variance']) / \\\n",
    "                      max(robot_features['motion_variance'], video_features['motion_variance'], 1e-6)\n",
    "        video_sim['motion_variance'] = max(0, variance_sim)\n",
    "        \n",
    "        # Temporal dynamics similarity\n",
    "        if 'temporal_dynamics' in video_features:\n",
    "            rhythm_sim = 1.0 - abs(robot_features['temporal_dynamics']['rhythm_strength'] - \n",
    "                                 video_features['temporal_dynamics']['rhythm_strength']) / \\\n",
    "                         max(robot_features['temporal_dynamics']['rhythm_strength'], \n",
    "                            video_features['temporal_dynamics']['rhythm_strength'], 1e-6)\n",
    "            video_sim['rhythm_similarity'] = max(0, rhythm_sim)\n",
    "            similarities.append(rhythm_sim)\n",
    "        \n",
    "        style_similarity_results['video_similarity'] = video_sim\n",
    "        similarities.extend([motion_sim, variance_sim])\n",
    "    \n",
    "    if pose_features:\n",
    "        pose_sim = {}\n",
    "        \n",
    "        # Rhythm intensity similarity\n",
    "        rhythm_sim = 1.0 - abs(robot_features['rhythm_intensity'] - pose_features['rhythm_intensity']) / \\\n",
    "                     max(robot_features['rhythm_intensity'], pose_features['rhythm_intensity'], 1e-6)\n",
    "        pose_sim['rhythm_intensity'] = max(0, rhythm_sim)\n",
    "        \n",
    "        # Acceleration similarity\n",
    "        accel_sim = 1.0 - abs(robot_features['acceleration_intensity'] - pose_features['acceleration_intensity']) / \\\n",
    "                    max(robot_features['acceleration_intensity'], pose_features['acceleration_intensity'], 1e-6)\n",
    "        pose_sim['acceleration_intensity'] = max(0, accel_sim)\n",
    "        \n",
    "        # Coordination similarity\n",
    "        coord_sim = 1.0 - abs(robot_features['coordination_score'] - pose_features['coordination_score']) / \\\n",
    "                    max(robot_features['coordination_score'], pose_features['coordination_score'], 1e-6)\n",
    "        pose_sim['coordination'] = max(0, coord_sim)\n",
    "        \n",
    "        # Movement range similarity\n",
    "        range_sim = 1.0 - abs(robot_features['movement_range'] - pose_features['movement_range']) / \\\n",
    "                    max(robot_features['movement_range'], pose_features['movement_range'], 1e-6)\n",
    "        pose_sim['movement_range'] = max(0, range_sim)\n",
    "        \n",
    "        # Temporal consistency similarity\n",
    "        if 'temporal_consistency' in pose_features:\n",
    "            fluidity_sim = 1.0 - abs(robot_features['temporal_consistency']['movement_fluidity'] - \n",
    "                                   pose_features['temporal_consistency']['movement_fluidity'])\n",
    "            pose_sim['fluidity'] = max(0, fluidity_sim)\n",
    "            similarities.append(fluidity_sim)\n",
    "        \n",
    "        style_similarity_results['pose_similarity'] = pose_sim\n",
    "        similarities.extend([rhythm_sim, accel_sim, coord_sim, range_sim])\n",
    "    \n",
    "    # Calculate overall style match\n",
    "    if similarities:\n",
    "        style_similarity_results['overall_style_match'] = np.mean(similarities)\n",
    "    \n",
    "    return style_similarity_results\n",
    "\n",
    "def evaluate_style_transfer_with_reference(original_csv_path, styled_csv_path, style_name=None, reference_video_path=None):\n",
    "    print(f\"Evaluating style transfer with reference style comparison:\")\n",
    "    print(f\"  Original: {os.path.basename(original_csv_path)}\")\n",
    "    print(f\"  Styled: {os.path.basename(styled_csv_path)}\")\n",
    "    print(f\"  Target style: {style_name if style_name else 'Unknown'}\")\n",
    "    \n",
    "    base_results = evaluate_style_transfer_comprehensive(original_csv_path, styled_csv_path)\n",
    "    \n",
    "    if not base_results:\n",
    "        return None\n",
    "    \n",
    "    # Load robot motions\n",
    "    try:\n",
    "        original_robot = np.loadtxt(original_csv_path, delimiter=',', skiprows=1)\n",
    "        styled_robot = np.loadtxt(styled_csv_path, delimiter=',', skiprows=1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading robot motions: {e}\")\n",
    "        return base_results\n",
    "    \n",
    "    # Ensure same dimensions\n",
    "    min_length = min(len(original_robot), len(styled_robot))\n",
    "    min_joints = min(original_robot.shape[1], styled_robot.shape[1])\n",
    "    original_robot = original_robot[:min_length, :min_joints]\n",
    "    styled_robot = styled_robot[:min_length, :min_joints]\n",
    "    \n",
    "    # Extract style features from robot motions\n",
    "    original_robot_features = extract_robot_motion_style_features(original_robot)\n",
    "    styled_robot_features = extract_robot_motion_style_features(styled_robot)\n",
    "    \n",
    "    # Load reference style data\n",
    "    reference_video_features = None\n",
    "    reference_pose_features = None\n",
    "    \n",
    "    if style_name:\n",
    "        print(f\"Loading reference data for style: {style_name}\")\n",
    "        reference_video_features, reference_pose_features = load_reference_style_data(style_name)\n",
    "        \n",
    "        if reference_video_features or reference_pose_features:\n",
    "            print(\"âœ“ Reference style data loaded successfully\")\n",
    "        else:\n",
    "            print(\"âš  Could not load reference style data\")\n",
    "    \n",
    "    # Compare styled robot with reference style\n",
    "    style_comparison = compare_style_features(\n",
    "        styled_robot_features, \n",
    "        reference_video_features, \n",
    "        reference_pose_features, \n",
    "        style_name\n",
    "    )\n",
    "    \n",
    "    # Compare original robot with reference style (baseline)\n",
    "    original_style_comparison = compare_style_features(\n",
    "        original_robot_features,\n",
    "        reference_video_features,\n",
    "        reference_pose_features,\n",
    "        style_name\n",
    "    )\n",
    "    \n",
    "    # Add style comparison results to base results\n",
    "    base_results['style_feature_analysis'] = {\n",
    "        'styled_robot_features': styled_robot_features,\n",
    "        'original_robot_features': original_robot_features,\n",
    "        'reference_video_features': reference_video_features,\n",
    "        'reference_pose_features': reference_pose_features,\n",
    "        'styled_vs_reference': style_comparison,\n",
    "        'original_vs_reference': original_style_comparison,\n",
    "        'style_transfer_improvement': style_comparison['overall_style_match'] - original_style_comparison['overall_style_match']\n",
    "    }\n",
    "    \n",
    "    return base_results\n",
    "\n",
    "def print_enhanced_evaluation(results):\n",
    "    print_comprehensive_evaluation(results)\n",
    "    \n",
    "    # Add style feature analysis\n",
    "    if 'style_feature_analysis' in results:\n",
    "        style_analysis = results['style_feature_analysis']\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\" STYLE FEATURE ANALYSIS - REFERENCE COMPARISON\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        styled_comparison = style_analysis['styled_vs_reference']\n",
    "        original_comparison = style_analysis['original_vs_reference']\n",
    "        improvement = style_analysis['style_transfer_improvement']\n",
    "        \n",
    "        print(f\"\\n Target Style: {styled_comparison['style_name']}\")\n",
    "        print(f\" Style Match Score: {styled_comparison['overall_style_match']:.3f}\")\n",
    "        print(f\" Baseline (Original): {original_comparison['overall_style_match']:.3f}\")\n",
    "        print(f\" Style Transfer Improvement: {improvement:.3f}\")\n",
    "        \n",
    "        if improvement > 0.2:\n",
    "            print(\"  EXCELLENT - Significant style characteristics captured\")\n",
    "        elif improvement > 0.1:\n",
    "            print(\"  GOOD - Noticeable style characteristics captured\")\n",
    "        elif improvement > 0.05:\n",
    "            print(\"  MODERATE - Some style characteristics captured\")\n",
    "        else:\n",
    "            print(\"  MINIMAL - Limited style characteristics captured\")\n",
    "        \n",
    "        # Video similarity details\n",
    "        if styled_comparison['video_similarity']:\n",
    "            print(f\"\\n Video Style Similarity:\")\n",
    "            video_sim = styled_comparison['video_similarity']\n",
    "            for feature, score in video_sim.items():\n",
    "                print(f\"   {feature.replace('_', ' ').title()}: {score:.3f}\")\n",
    "        \n",
    "        # Pose similarity details\n",
    "        if styled_comparison['pose_similarity']:\n",
    "            print(f\"\\n Pose Style Similarity:\")\n",
    "            pose_sim = styled_comparison['pose_similarity']\n",
    "            for feature, score in pose_sim.items():\n",
    "                print(f\"   {feature.replace('_', ' ').title()}: {score:.3f}\")\n",
    "        \n",
    "        # Reference vs styled feature comparison\n",
    "        if style_analysis['reference_video_features'] and style_analysis['reference_pose_features']:\n",
    "            print(f\"\\n Style Characteristic Comparison:\")\n",
    "            styled_features = style_analysis['styled_robot_features']\n",
    "            ref_video = style_analysis['reference_video_features']\n",
    "            ref_pose = style_analysis['reference_pose_features']\n",
    "            \n",
    "            print(f\"   Motion Intensity - Robot: {styled_features['motion_intensity']:.3f}, Video: {ref_video['motion_intensity']:.3f}\")\n",
    "            print(f\"   Rhythm Intensity - Robot: {styled_features['rhythm_intensity']:.3f}, Pose: {ref_pose['rhythm_intensity']:.3f}\")\n",
    "            print(f\"   Coordination - Robot: {styled_features['coordination_score']:.3f}, Pose: {ref_pose['coordination_score']:.3f}\")\n",
    "            print(f\"   Movement Range - Robot: {styled_features['movement_range']:.3f}, Pose: {ref_pose['movement_range']:.3f}\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "\n",
    "\n",
    "def print_comprehensive_evaluation(results):    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" COMPREHENSIVE ROBOTIC STYLE TRANSFER EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Overall Quality\n",
    "    overall = results['overall_assessment']['overall_quality']\n",
    "    print(f\"\\n Overall Quality Score: {overall:.3f}\")\n",
    "    if overall > 0.8:\n",
    "        print(\"  EXCELLENT - Great style transfer!\")\n",
    "    elif overall > 0.6:\n",
    "        print(\"  GOOD - Effective style transfer\") \n",
    "    elif overall > 0.4:\n",
    "        print(\"   FAIR - Some style transfer detected\")\n",
    "    else:\n",
    "        print(\"  POOR - Limited effective style transfer\")\n",
    "    \n",
    "    # Trajectory Analysis\n",
    "    trajectory = results['trajectory_analysis']\n",
    "    func_score = trajectory['functional_preservation']['overall_score']\n",
    "    print(f\"\\n Functional Preservation: {func_score:.3f}\")\n",
    "    if func_score > 0.8:\n",
    "        print(\"  EXCELLENT - Robot maintains excellent functionality\")\n",
    "    elif func_score > 0.6:\n",
    "        print(\"  GOOD - Robot maintains good functionality\")\n",
    "    elif func_score > 0.4:\n",
    "        print(\"   FAIR - Robot has moderate functional issues\")\n",
    "    else:\n",
    "        print(\"  POOR - Robot has significant functional issues\")\n",
    "    \n",
    "    # trajectory metrics\n",
    "    acc = trajectory['absolute_accuracy']\n",
    "    print(f\"\\n   Position Accuracy:\")\n",
    "    print(f\"     Average MAE: {acc['average_mae']:.3f} radians\")\n",
    "    print(f\"     Max Error: {acc['max_error_across_joints']:.3f} radians\")\n",
    "    \n",
    "    if acc['average_mae'] < 0.1:\n",
    "        print(\"     EXCELLENT - Very accurate positioning\")\n",
    "    elif acc['average_mae'] < 0.3:\n",
    "        print(\"     GOOD - Acceptable positioning\")\n",
    "    elif acc['average_mae'] < 0.5:\n",
    "        print(\"     FAIR - Moderate positioning errors\")\n",
    "    else:\n",
    "        print(\"     POOR - Significant positioning errors\")\n",
    "    \n",
    "    pattern = trajectory['pattern_similarity']\n",
    "    print(f\"\\n   Pattern Similarity: {pattern['average_correlation']:.3f}\")\n",
    "    if pattern['average_correlation'] > 0.8:\n",
    "        print(\"     EXCELLENT - Motion patterns very similar\")\n",
    "    elif pattern['average_correlation'] > 0.6:\n",
    "        print(\"     GOOD - Motion patterns mostly similar\")\n",
    "    else:\n",
    "        print(\"     NEEDS IMPROVEMENT - Patterns significantly different\")\n",
    "    \n",
    "    workspace = trajectory['workspace_preservation']\n",
    "    print(f\"\\n   Workspace Preservation: {workspace['total_violation']:.3f} violation\")\n",
    "    if workspace['total_violation'] < 0.1:\n",
    "        print(\"     EXCELLENT - Stays within workspace\")\n",
    "    elif workspace['total_violation'] < 0.5:\n",
    "        print(\"     GOOD - Minor workspace violations\")\n",
    "    else:\n",
    "        print(\"     POOR - Significant workspace violations\")\n",
    "    \n",
    "    # Physical Deviation\n",
    "    deviation = results['physical_deviation']\n",
    "    print(f\"\\n Physical Deviation: {deviation['average']:.3f} radians (max: {deviation['maximum']:.3f})\")\n",
    "    if deviation['average'] < 0.2:\n",
    "        print(\"  MINIMAL - Robot stays very close to original path\")\n",
    "    elif deviation['average'] < 0.5:\n",
    "        print(\"  ACCEPTABLE - Moderate deviation from original path\")\n",
    "    else:\n",
    "        print(\"  HIGH - Significant deviation from original path\")\n",
    "    \n",
    "    # Style Characteristics\n",
    "    style = results['style_characteristics']\n",
    "    print(f\"\\n Style Characteristics Applied:\")\n",
    "    print(f\"   Speed change: {style['speed_change']:.3f} - {'More dynamic' if style['speed_change'] > 0.2 else 'Similar tempo'}\")\n",
    "    print(f\"   Acceleration change: {style['acceleration_change']:.3f} - {'Sharp rhythmic patterns' if style['acceleration_change'] > 0.5 else 'Gentle acceleration changes'}\")\n",
    "    print(f\"   Coordination change: {style['coordination_change']:.3f} - {'Different joint coordination' if style['coordination_change'] > 0.2 else 'Similar coordination'}\")\n",
    "    print(f\"   Movement range change: {style['range_change']:.3f} - {'Different movement extent' if style['range_change'] > 0.2 else 'Similar range'}\")\n",
    "    print(f\"   Overall style transfer: {style['overall_style_change']:.3f}\")\n",
    "    \n",
    "    if style['overall_style_change'] > 0.3:\n",
    "        print(\"  STRONG - Clear style characteristics applied\")\n",
    "    elif style['overall_style_change'] > 0.1:\n",
    "        print(\"  MODERATE - Noticeable style characteristics applied\")\n",
    "    else:\n",
    "        print(\"  MINIMAL - Very little style transfer detected\")\n",
    "    \n",
    "    # Motion Quality\n",
    "    quality = results['motion_quality']\n",
    "    print(f\"\\n Motion Smoothness: {quality['smoothness_ratio']:.3f}\")\n",
    "    if quality['smoothness_ratio'] < 1.2:\n",
    "        print(\"  EXCELLENT - Motion remains smooth\")\n",
    "    elif quality['smoothness_ratio'] < 2.0:\n",
    "        print(\"  FAIR - Noticeable decrease in smoothness\")\n",
    "    else:\n",
    "        print(\"  POOR - Significant decrease in smoothness\")\n",
    "    \n",
    "    # Frequency Analysis\n",
    "    freq = results['frequency_analysis']\n",
    "    print(f\"\\n Frequency Content Change: {freq['frequency_change']:.3f}\")\n",
    "    if freq['frequency_change'] > 0.3:\n",
    "        print(\"  SIGNIFICANT - Notable rhythmic changes\")\n",
    "    elif freq['frequency_change'] > 0.1:\n",
    "        print(\"  MODERATE - Some rhythmic changes\")\n",
    "    else:\n",
    "        print(\"  MINIMAL - Little rhythmic change\")\n",
    "    \n",
    "    # Per-Joint Analysis\n",
    "    print(f\"\\n Per-Joint Analysis:\")\n",
    "    mae_vals = trajectory['absolute_accuracy']['joint_mae']\n",
    "    corr_vals = trajectory['pattern_similarity']['joint_correlations']\n",
    "    deviations = results['physical_deviation']['per_joint']\n",
    "    \n",
    "    for i, (mae, corr, dev) in enumerate(zip(mae_vals, corr_vals, deviations)):\n",
    "        status = \"âœ“\" if mae < 0.3 and corr > 0.6 else \"âš \" if mae < 0.5 else \"âœ—\"\n",
    "        print(f\"   Joint {i+1}: MAE={mae:.3f}, correlation={corr:.3f}, deviation={dev:.3f} {status}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "def run_enhanced_evaluation(original_csv_path, styled_csv_path, style_name=None):\n",
    "    results = evaluate_style_transfer_with_reference(original_csv_path, styled_csv_path, style_name)\n",
    "    \n",
    "    if results:\n",
    "        # Load robot data for visualisation\n",
    "        original_robot = np.loadtxt(original_csv_path, delimiter=',', skiprows=1)\n",
    "        styled_robot = np.loadtxt(styled_csv_path, delimiter=',', skiprows=1)\n",
    "        \n",
    "        # Ensure same dimensions\n",
    "        min_length = min(len(original_robot), len(styled_robot))\n",
    "        min_joints = min(original_robot.shape[1], styled_robot.shape[1])\n",
    "        original_robot = original_robot[:min_length, :min_joints]\n",
    "        styled_robot = styled_robot[:min_length, :min_joints]\n",
    "        \n",
    "        print_enhanced_evaluation(results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8967ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_enhanced_evaluation(\n",
    "    \"/Users/pszkw/robot_motion_data/robot_motions/robot_Senario5_KatWellyTogether_Welly.csv\", \n",
    "    \"temporal_styled_Flamenco_c0.25_temporal_cross_attn_robot_Senario5_KatWellyTogether_Welly.csv\", \n",
    "    style_name=\"Flamenco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e12b118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
